{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from mylib.models import densesharp, metrics, losses,densenet\n",
    "from mylib.utils.misc import rotation, reflection, crop, random_center, _triple\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam,Adamax,Nadam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv3D, MaxPool3D\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "class MixupGenerator():\n",
    "    def __init__(self, X_train, y_train, batch_size=32, alpha=0.2, shuffle=True, datagen=None):\n",
    "        X_train_new = []\n",
    "        for i in range(X_train.shape[0]):\n",
    "                try:\n",
    "                    X_train_new = np.append(X_train_new, np.expand_dims(Transform(32,5)(X_train[i]), axis=0), axis=0)                  \n",
    "                except ValueError:\n",
    "                    X_train_new = np.expand_dims(Transform(32,5)(X_train[0]), axis=0)\n",
    "        self.X_train = X_train_new\n",
    "        self.y_train = y_train\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.shuffle = shuffle\n",
    "        self.sample_num = len(X_train)\n",
    "        self.datagen = datagen\n",
    "\n",
    "    def __call__(self):\n",
    "        while True:\n",
    "            indexes = self.__get_exploration_order()\n",
    "            itr_num = int(len(indexes) // (self.batch_size * 2))\n",
    "\n",
    "            for i in range(itr_num):\n",
    "                batch_ids = indexes[i * self.batch_size * 2:(i + 1) * self.batch_size * 2]\n",
    "                X, y = self.__data_generation(batch_ids)\n",
    "\n",
    "                yield X, y\n",
    "\n",
    "    def __get_exploration_order(self):\n",
    "        indexes = np.arange(self.sample_num)\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "\n",
    "        return indexes\n",
    "\n",
    "    def __data_generation(self, batch_ids):\n",
    "        _, h, w, c = self.X_train.shape\n",
    "        l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "        X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
    "        y_l = l.reshape(self.batch_size, 1)\n",
    "\n",
    "        X1 = self.X_train[batch_ids[:self.batch_size]]\n",
    "        X2 = self.X_train[batch_ids[self.batch_size:]]\n",
    "        X = X1 * X_l + X2 * (1 - X_l)\n",
    "        X = np.expand_dims(X,axis=-1)\n",
    "\n",
    "        if self.datagen:\n",
    "            for i in range(self.batch_size):\n",
    "                X[i] = self.datagen.random_transform(X[i])\n",
    "                X[i] = self.datagen.standardize(X[i])\n",
    "\n",
    "        if isinstance(self.y_train, list):\n",
    "            y = []\n",
    "\n",
    "            for y_train_ in self.y_train:\n",
    "                y1 = y_train_[batch_ids[:self.batch_size]]\n",
    "                y2 = y_train_[batch_ids[self.batch_size:]]\n",
    "                y.append(y1 * y_l + y2 * (1 - y_l))\n",
    "        else:\n",
    "            y1 = self.y_train[batch_ids[:self.batch_size]]\n",
    "            y2 = self.y_train[batch_ids[self.batch_size:]]\n",
    "            y = y1 * y_l + y2 * (1 - y_l)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "class Transform:\n",
    "    '''The online data augmentation, including:\n",
    "    1) random move the center by `move`\n",
    "    2) rotation 90 degrees increments\n",
    "    3) reflection in any axis\n",
    "    '''\n",
    "\n",
    "    def __init__(self, size, move):\n",
    "        self.size = _triple(size)\n",
    "        self.move = move\n",
    "\n",
    "    def __call__(self, arr, aux=None):\n",
    "        shape = arr.shape\n",
    "        if self.move is not None:\n",
    "            center = random_center(shape, self.move)\n",
    "            arr_ret = crop(arr, center, self.size)\n",
    "            angle = np.random.randint(4, size=3)\n",
    "            arr_ret = rotation(arr_ret, angle=angle)\n",
    "            axis = np.random.randint(4) - 1\n",
    "            arr_ret = reflection(arr_ret, axis=axis)\n",
    "            #arr_ret = np.expand_dims(arr_ret, axis=0)\n",
    "            if aux is not None:\n",
    "                aux_ret = crop(aux, center, self.size)\n",
    "                aux_ret = rotation(aux_ret, angle=angle)\n",
    "                aux_ret = reflection(aux_ret, axis=axis)\n",
    "                aux_ret = np.expand_dims(aux_ret, axis=0)\n",
    "                return arr_ret, aux_ret\n",
    "            return arr_ret\n",
    "        else:\n",
    "            center = np.array(shape) // 2\n",
    "            arr_ret = crop(arr, center, self.size)\n",
    "            arr_ret = np.expand_dims(arr_ret, axis=0)\n",
    "            if aux is not None:\n",
    "                aux_ret = crop(aux, center, self.size)\n",
    "                #aux_ret = np.expand_dims(aux_ret, axis=0)\n",
    "                return arr_ret, aux_ret\n",
    "            return arr_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading: 100%|██████████| 584/584 [00:19<00:00, 30.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 32, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "voxel_train = []\n",
    "seg_train = []\n",
    "for i in tqdm(range(584), desc='reading'):\n",
    "    try:\n",
    "        data = np.load('data/train_val/candidate{}.npz'.format(i))\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    try:\n",
    "        voxel_train = np.append(voxel_train, np.expand_dims(data['voxel'], axis=0), axis=0)\n",
    "        seg_train = np.append(seg_train, np.expand_dims(data['seg'], axis=0), axis=0)\n",
    "    except ValueError:\n",
    "        voxel_train = np.expand_dims(data['voxel'], axis=0)\n",
    "        seg_train = np.expand_dims(data['seg'], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "#voxel_train=voxel_train.astype('float32')\n",
    "training_batch_size = voxel_train.shape[0]#465\n",
    "train_label = pd.read_csv('data/train_val.csv').values[:, 1].astype(int)\n",
    "train_label = to_categorical(train_label, 2)\n",
    "voxel_train=voxel_train*seg_train\n",
    "x_train,x_val,y_train, y_val =train_test_split(voxel_train,train_label,test_size=100,shuffle=True,stratify=train_label)\n",
    "tmp_val=[]\n",
    "tmp_val = np.expand_dims(crop(x_val[0],(50,50,50),(32,32,32)),axis=0)\n",
    "for i in range(x_val.shape[0]-1):\n",
    "    tmp_val = np.append(tmp_val,np.expand_dims(crop(x_val[i+1],(50,50,50),(32,32,32)),axis=0),axis=0)\n",
    "x_val = tmp_val\n",
    "x_val = np.expand_dims(x_val,axis=-1)\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading test_data: 100%|██████████| 584/584 [00:01<00:00, 345.58it/s]\n",
      "croping: 100%|██████████| 116/116 [00:00<00:00, 3036.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 32, 32)\n",
      "(117, 32, 32, 32)\n",
      "(117, 32, 32, 32, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#读取测试数据\n",
    "voxel_test = []\t\t#设置测试数据的voxel_test\n",
    "seg_test = []\t\t#设置测试数据的seg_test\n",
    "\n",
    "for i in tqdm(range(584), desc='reading test_data'):\t#展示写入测试数据的进度\n",
    "    try:\n",
    "        tmp = np.load('data/test/candidate{}.npz'.format(i))\t#依次读取测试数据中的candidate{i}文件\n",
    "    except FileNotFoundError:\t\t\t\t\t\t\t\t\t#无该文件时直接进入下一次循环\n",
    "        continue\n",
    "    try:\n",
    "        voxel_test = np.append(voxel_test, np.expand_dims(tmp['voxel'], axis=0), axis=0)\t#向voxel_test中添加读取的voxel向量，但是初次读取会出错\n",
    "        seg_test = np.append(seg_test, np.expand_dims(tmp['seg'], axis=0), axis=0)\t\t\t#向seg_test中添加读取的seg向量，同样初次读取时会出错\n",
    "    except ValueError:\n",
    "        voxel_test = np.expand_dims(tmp['voxel'], axis=0)\t#向空矩阵中写入初次读取的voxel\t量\n",
    "        seg_test = np.expand_dims(tmp['seg'], axis=0)\t#向空矩阵中写入初次读取的seg量\n",
    "\n",
    "seg_test = seg_test.astype(int)         #将seg布尔array转换为1/0整数\n",
    "X_test= voxel_test*seg_test          #将结节抠出来\n",
    "\n",
    "\n",
    "X_test=X_test.astype(np.float32)\n",
    "#X_test=X_test/128.-1.\n",
    "#X_test = np.concatenate((X_test,np.transpose(X_test,(0,2,1,3))),axis=0)    #将训练集的xy转置得到新数据集以扩充数据\n",
    "#print(X_test.shape)\n",
    "training_test_size = X_test.shape[0]  #训练数据集的数量\n",
    "X_test_new=crop(X_test[0],(50,50,50),(32,32,32))\n",
    "\n",
    "X_test_new=np.expand_dims(X_test_new,axis=0)\n",
    "print(X_test_new.shape) \n",
    "test_batch_size = X_test.shape[0]\n",
    "for i in tqdm(range(test_batch_size-1),desc='croping'):\n",
    "    X_test_new=np.append(X_test_new,np.expand_dims(crop(X_test[i+1],(50,50,50),(32,32,32)),axis=0),axis=0)\n",
    "print(X_test_new.shape)   \n",
    "del X_test\n",
    "X_test_new = X_test_new.reshape(X_test_new.shape[0], 32, 32, 32, 1)     #将训练数据集整合成5d张量\n",
    "print(X_test_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hyper-parameters: {'activation': <function <lambda> at 0x7ff3fc8a20e0>, 'bn_scale': True, 'weight_decay': 0.0, 'kernel_initializer': 'he_uniform', 'first_scale': <function <lambda> at 0x7ff3fc8a25f0>, 'dhw': [32, 32, 32], 'k': 16, 'bottleneck': 4, 'compression': 2, 'first_layer': 32, 'down_structure': [4, 4, 4], 'output_size': 2}\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 32, 1 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 32, 32, 32, 1 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 32, 32, 32, 3 896         lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 32, 3 128         conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 32, 3 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 32, 32, 32, 6 2048        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 32, 6 256         conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 32, 6 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 32, 32, 32, 1 27664       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 32, 4 0           conv3d_3[0][0]                   \n",
      "                                                                 conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 32, 4 192         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 32, 4 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 32, 32, 32, 6 3072        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32, 6 256         conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32, 6 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)               (None, 32, 32, 32, 1 27664       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 32, 6 0           conv3d_5[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32, 6 256         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32, 6 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)               (None, 32, 32, 32, 6 4096        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 32, 6 256         conv3d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 32, 6 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)               (None, 32, 32, 32, 1 27664       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 32, 8 0           conv3d_7[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 32, 8 320         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 32, 8 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_8 (Conv3D)               (None, 32, 32, 32, 6 5120        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 32, 6 256         conv3d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 32, 6 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_9 (Conv3D)               (None, 32, 32, 32, 1 27664       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 32, 9 0           conv3d_9[0][0]                   \n",
      "                                                                 concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 32, 9 384         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 32, 9 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_10 (Conv3D)              (None, 32, 32, 32, 4 4656        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling3d_1 (AveragePoo (None, 16, 16, 16, 4 0           conv3d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 16, 4 192         average_pooling3d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 16, 4 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_11 (Conv3D)              (None, 16, 16, 16, 6 3072        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 16, 6 256         conv3d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 16, 6 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_12 (Conv3D)              (None, 16, 16, 16, 1 27664       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 16, 16, 16, 6 0           conv3d_12[0][0]                  \n",
      "                                                                 average_pooling3d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 16, 6 256         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 16, 6 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_13 (Conv3D)              (None, 16, 16, 16, 6 4096        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 16, 6 256         conv3d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 16, 6 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_14 (Conv3D)              (None, 16, 16, 16, 1 27664       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 16, 16, 16, 8 0           conv3d_14[0][0]                  \n",
      "                                                                 concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 16, 8 320         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 16, 8 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_15 (Conv3D)              (None, 16, 16, 16, 6 5120        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 16, 6 256         conv3d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 16, 6 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_16 (Conv3D)              (None, 16, 16, 16, 1 27664       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 16, 16, 16, 9 0           conv3d_16[0][0]                  \n",
      "                                                                 concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 16, 9 384         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 16, 9 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_17 (Conv3D)              (None, 16, 16, 16, 6 6144        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 16, 6 256         conv3d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 16, 6 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_18 (Conv3D)              (None, 16, 16, 16, 1 27664       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 16, 16, 16, 1 0           conv3d_18[0][0]                  \n",
      "                                                                 concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 16, 1 448         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 16, 1 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_19 (Conv3D)              (None, 16, 16, 16, 5 6328        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling3d_2 (AveragePoo (None, 8, 8, 8, 56)  0           conv3d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 8, 56)  224         average_pooling3d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 8, 56)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_20 (Conv3D)              (None, 8, 8, 8, 64)  3584        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 8, 64)  256         conv3d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 8, 64)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_21 (Conv3D)              (None, 8, 8, 8, 16)  27664       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 8, 8, 8, 72)  0           conv3d_21[0][0]                  \n",
      "                                                                 average_pooling3d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 8, 72)  288         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 8, 72)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_22 (Conv3D)              (None, 8, 8, 8, 64)  4608        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 8, 64)  256         conv3d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 8, 8, 8, 64)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_23 (Conv3D)              (None, 8, 8, 8, 16)  27664       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 8, 8, 8, 88)  0           conv3d_23[0][0]                  \n",
      "                                                                 concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 8, 8, 8, 88)  352         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 8, 8, 8, 88)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_24 (Conv3D)              (None, 8, 8, 8, 64)  5632        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 8, 8, 8, 64)  256         conv3d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 8, 8, 8, 64)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_25 (Conv3D)              (None, 8, 8, 8, 16)  27664       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 8, 8, 8, 104) 0           conv3d_25[0][0]                  \n",
      "                                                                 concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 8, 8, 8, 104) 416         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 8, 8, 8, 104) 0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_26 (Conv3D)              (None, 8, 8, 8, 64)  6656        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 8, 8, 8, 64)  256         conv3d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 8, 8, 8, 64)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_27 (Conv3D)              (None, 8, 8, 8, 16)  27664       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 8, 8, 8, 120) 0           conv3d_27[0][0]                  \n",
      "                                                                 concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 8, 120) 480         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 8, 120) 0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling3d_1 (Glo (None, 120)          0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            242         global_average_pooling3d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 405,050\n",
      "Trainable params: 401,194\n",
      "Non-trainable params: 3,856\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.7736 - categorical_crossentropy: 0.7736 - categorical_accuracy: 0.6250 - val_loss: 0.9320 - val_categorical_crossentropy: 0.9320 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00001: saving model to tmp/test/weights.01.h5\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.53000, saving model to tmp/test/best_acc.h5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.93203, saving model to tmp/test/best_loss.h5\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 7s 619ms/step - loss: 0.7859 - categorical_crossentropy: 0.7859 - categorical_accuracy: 0.6120 - val_loss: 0.9252 - val_categorical_crossentropy: 0.9252 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00002: saving model to tmp/test/weights.02.h5\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.93203 to 0.92516, saving model to tmp/test/best_loss.h5\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 7s 619ms/step - loss: 0.7258 - categorical_crossentropy: 0.7258 - categorical_accuracy: 0.6276 - val_loss: 0.9319 - val_categorical_crossentropy: 0.9319 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00003: saving model to tmp/test/weights.03.h5\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.92516\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 7s 619ms/step - loss: 0.7604 - categorical_crossentropy: 0.7604 - categorical_accuracy: 0.6302 - val_loss: 0.9285 - val_categorical_crossentropy: 0.9285 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00004: saving model to tmp/test/weights.04.h5\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.92516\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7194 - categorical_crossentropy: 0.7194 - categorical_accuracy: 0.6510 - val_loss: 0.9242 - val_categorical_crossentropy: 0.9242 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00005: saving model to tmp/test/weights.05.h5\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.92516 to 0.92425, saving model to tmp/test/best_loss.h5\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.6570 - categorical_crossentropy: 0.6570 - categorical_accuracy: 0.6641 - val_loss: 0.9253 - val_categorical_crossentropy: 0.9253 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00006: saving model to tmp/test/weights.06.h5\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.92425\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 7s 619ms/step - loss: 0.7139 - categorical_crossentropy: 0.7139 - categorical_accuracy: 0.6536 - val_loss: 0.9245 - val_categorical_crossentropy: 0.9245 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00007: saving model to tmp/test/weights.07.h5\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.92425\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7387 - categorical_crossentropy: 0.7387 - categorical_accuracy: 0.6328 - val_loss: 0.9270 - val_categorical_crossentropy: 0.9270 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00008: saving model to tmp/test/weights.08.h5\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.92425\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7487 - categorical_crossentropy: 0.7487 - categorical_accuracy: 0.6172 - val_loss: 0.9277 - val_categorical_crossentropy: 0.9277 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00009: saving model to tmp/test/weights.09.h5\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.92425\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7344 - categorical_crossentropy: 0.7344 - categorical_accuracy: 0.6510 - val_loss: 0.9277 - val_categorical_crossentropy: 0.9277 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.2578966624232636e-07.\n",
      "\n",
      "Epoch 00010: saving model to tmp/test/weights.10.h5\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.92425\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7920 - categorical_crossentropy: 0.7920 - categorical_accuracy: 0.6302 - val_loss: 0.9247 - val_categorical_crossentropy: 0.9247 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00011: saving model to tmp/test/weights.11.h5\n",
      "\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.92425\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 7s 617ms/step - loss: 0.7514 - categorical_crossentropy: 0.7514 - categorical_accuracy: 0.6172 - val_loss: 0.9266 - val_categorical_crossentropy: 0.9266 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00012: saving model to tmp/test/weights.12.h5\n",
      "\n",
      "Epoch 00012: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.92425\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7313 - categorical_crossentropy: 0.7313 - categorical_accuracy: 0.6484 - val_loss: 0.9260 - val_categorical_crossentropy: 0.9260 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00013: saving model to tmp/test/weights.13.h5\n",
      "\n",
      "Epoch 00013: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.92425\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.6855 - categorical_crossentropy: 0.6855 - categorical_accuracy: 0.6484 - val_loss: 0.9281 - val_categorical_crossentropy: 0.9281 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00014: saving model to tmp/test/weights.14.h5\n",
      "\n",
      "Epoch 00014: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.92425\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 7s 619ms/step - loss: 0.7709 - categorical_crossentropy: 0.7709 - categorical_accuracy: 0.6042 - val_loss: 0.9300 - val_categorical_crossentropy: 0.9300 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00015: saving model to tmp/test/weights.15.h5\n",
      "\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.92425\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7013 - categorical_crossentropy: 0.7013 - categorical_accuracy: 0.6667 - val_loss: 0.9288 - val_categorical_crossentropy: 0.9288 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.55990334589751e-08.\n",
      "\n",
      "Epoch 00016: saving model to tmp/test/weights.16.h5\n",
      "\n",
      "Epoch 00016: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.92425\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7464 - categorical_crossentropy: 0.7464 - categorical_accuracy: 0.6198 - val_loss: 0.9291 - val_categorical_crossentropy: 0.9291 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00017: saving model to tmp/test/weights.17.h5\n",
      "\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.92425\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7115 - categorical_crossentropy: 0.7115 - categorical_accuracy: 0.6354 - val_loss: 0.9285 - val_categorical_crossentropy: 0.9285 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00018: saving model to tmp/test/weights.18.h5\n",
      "\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.92425\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7689 - categorical_crossentropy: 0.7689 - categorical_accuracy: 0.6615 - val_loss: 0.9265 - val_categorical_crossentropy: 0.9265 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00019: saving model to tmp/test/weights.19.h5\n",
      "\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.92425\n",
      "Epoch 20/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7581 - categorical_crossentropy: 0.7581 - categorical_accuracy: 0.6354 - val_loss: 0.9267 - val_categorical_crossentropy: 0.9267 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00020: saving model to tmp/test/weights.20.h5\n",
      "\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.92425\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7298 - categorical_crossentropy: 0.7298 - categorical_accuracy: 0.6172 - val_loss: 0.9241 - val_categorical_crossentropy: 0.9241 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00021: saving model to tmp/test/weights.21.h5\n",
      "\n",
      "Epoch 00021: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.92425 to 0.92410, saving model to tmp/test/best_loss.h5\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7145 - categorical_crossentropy: 0.7145 - categorical_accuracy: 0.6771 - val_loss: 0.9236 - val_categorical_crossentropy: 0.9236 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00022: saving model to tmp/test/weights.22.h5\n",
      "\n",
      "Epoch 00022: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.92410 to 0.92358, saving model to tmp/test/best_loss.h5\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7131 - categorical_crossentropy: 0.7131 - categorical_accuracy: 0.6667 - val_loss: 0.9255 - val_categorical_crossentropy: 0.9255 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00023: saving model to tmp/test/weights.23.h5\n",
      "\n",
      "Epoch 00023: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.92358\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7544 - categorical_crossentropy: 0.7544 - categorical_accuracy: 0.6406 - val_loss: 0.9269 - val_categorical_crossentropy: 0.9269 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00024: saving model to tmp/test/weights.24.h5\n",
      "\n",
      "Epoch 00024: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.92358\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7563 - categorical_crossentropy: 0.7563 - categorical_accuracy: 0.6172 - val_loss: 0.9269 - val_categorical_crossentropy: 0.9269 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00025: saving model to tmp/test/weights.25.h5\n",
      "\n",
      "Epoch 00025: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.92358\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.6563 - categorical_crossentropy: 0.6563 - categorical_accuracy: 0.6667 - val_loss: 0.9253 - val_categorical_crossentropy: 0.9253 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00026: saving model to tmp/test/weights.26.h5\n",
      "\n",
      "Epoch 00026: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.92358\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7209 - categorical_crossentropy: 0.7209 - categorical_accuracy: 0.6510 - val_loss: 0.9251 - val_categorical_crossentropy: 0.9251 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.4574773341612398e-08.\n",
      "\n",
      "Epoch 00027: saving model to tmp/test/weights.27.h5\n",
      "\n",
      "Epoch 00027: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.92358\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.6960 - categorical_crossentropy: 0.6960 - categorical_accuracy: 0.6328 - val_loss: 0.9236 - val_categorical_crossentropy: 0.9236 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00028: saving model to tmp/test/weights.28.h5\n",
      "\n",
      "Epoch 00028: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.92358\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7276 - categorical_crossentropy: 0.7276 - categorical_accuracy: 0.6484 - val_loss: 0.9215 - val_categorical_crossentropy: 0.9215 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00029: saving model to tmp/test/weights.29.h5\n",
      "\n",
      "Epoch 00029: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.92358 to 0.92152, saving model to tmp/test/best_loss.h5\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 7s 619ms/step - loss: 0.7482 - categorical_crossentropy: 0.7482 - categorical_accuracy: 0.6276 - val_loss: 0.9219 - val_categorical_crossentropy: 0.9219 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00030: saving model to tmp/test/weights.30.h5\n",
      "\n",
      "Epoch 00030: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.92152\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7551 - categorical_crossentropy: 0.7551 - categorical_accuracy: 0.6120 - val_loss: 0.9233 - val_categorical_crossentropy: 0.9233 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00031: saving model to tmp/test/weights.31.h5\n",
      "\n",
      "Epoch 00031: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.92152\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.6948 - categorical_crossentropy: 0.6948 - categorical_accuracy: 0.6823 - val_loss: 0.9251 - val_categorical_crossentropy: 0.9251 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00032: saving model to tmp/test/weights.32.h5\n",
      "\n",
      "Epoch 00032: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.92152\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7439 - categorical_crossentropy: 0.7439 - categorical_accuracy: 0.6432 - val_loss: 0.9253 - val_categorical_crossentropy: 0.9253 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00033: saving model to tmp/test/weights.33.h5\n",
      "\n",
      "Epoch 00033: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.92152\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.6918 - categorical_crossentropy: 0.6918 - categorical_accuracy: 0.6771 - val_loss: 0.9272 - val_categorical_crossentropy: 0.9272 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0862050203286345e-08.\n",
      "\n",
      "Epoch 00034: saving model to tmp/test/weights.34.h5\n",
      "\n",
      "Epoch 00034: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.92152\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7335 - categorical_crossentropy: 0.7335 - categorical_accuracy: 0.6120 - val_loss: 0.9268 - val_categorical_crossentropy: 0.9268 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00035: saving model to tmp/test/weights.35.h5\n",
      "\n",
      "Epoch 00035: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.92152\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7290 - categorical_crossentropy: 0.7290 - categorical_accuracy: 0.6589 - val_loss: 0.9263 - val_categorical_crossentropy: 0.9263 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00036: saving model to tmp/test/weights.36.h5\n",
      "\n",
      "Epoch 00036: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.92152\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.6626 - categorical_crossentropy: 0.6626 - categorical_accuracy: 0.7005 - val_loss: 0.9275 - val_categorical_crossentropy: 0.9275 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00037: saving model to tmp/test/weights.37.h5\n",
      "\n",
      "Epoch 00037: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.92152\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.6918 - categorical_crossentropy: 0.6918 - categorical_accuracy: 0.6458 - val_loss: 0.9280 - val_categorical_crossentropy: 0.9280 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00038: saving model to tmp/test/weights.38.h5\n",
      "\n",
      "Epoch 00038: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.92152\n",
      "Epoch 39/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7953 - categorical_crossentropy: 0.7953 - categorical_accuracy: 0.6224 - val_loss: 0.9262 - val_categorical_crossentropy: 0.9262 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00039: saving model to tmp/test/weights.39.h5\n",
      "\n",
      "Epoch 00039: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.92152\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7667 - categorical_crossentropy: 0.7667 - categorical_accuracy: 0.6276 - val_loss: 0.9273 - val_categorical_crossentropy: 0.9273 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 4.8010260673692075e-09.\n",
      "\n",
      "Epoch 00040: saving model to tmp/test/weights.40.h5\n",
      "\n",
      "Epoch 00040: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.92152\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7138 - categorical_crossentropy: 0.7138 - categorical_accuracy: 0.6458 - val_loss: 0.9267 - val_categorical_crossentropy: 0.9267 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00041: saving model to tmp/test/weights.41.h5\n",
      "\n",
      "Epoch 00041: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.92152\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7740 - categorical_crossentropy: 0.7740 - categorical_accuracy: 0.6302 - val_loss: 0.9273 - val_categorical_crossentropy: 0.9273 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00042: saving model to tmp/test/weights.42.h5\n",
      "\n",
      "Epoch 00042: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.92152\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.6920 - categorical_crossentropy: 0.6920 - categorical_accuracy: 0.6745 - val_loss: 0.9274 - val_categorical_crossentropy: 0.9274 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00043: saving model to tmp/test/weights.43.h5\n",
      "\n",
      "Epoch 00043: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.92152\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7831 - categorical_crossentropy: 0.7831 - categorical_accuracy: 0.5964 - val_loss: 0.9262 - val_categorical_crossentropy: 0.9262 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00044: saving model to tmp/test/weights.44.h5\n",
      "\n",
      "Epoch 00044: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.92152\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7864 - categorical_crossentropy: 0.7864 - categorical_accuracy: 0.6250 - val_loss: 0.9257 - val_categorical_crossentropy: 0.9257 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00045: saving model to tmp/test/weights.45.h5\n",
      "\n",
      "Epoch 00045: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.92152\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7444 - categorical_crossentropy: 0.7444 - categorical_accuracy: 0.6250 - val_loss: 0.9265 - val_categorical_crossentropy: 0.9265 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 2.1220535995070124e-09.\n",
      "\n",
      "Epoch 00046: saving model to tmp/test/weights.46.h5\n",
      "\n",
      "Epoch 00046: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.92152\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 7s 617ms/step - loss: 0.7340 - categorical_crossentropy: 0.7340 - categorical_accuracy: 0.6589 - val_loss: 0.9258 - val_categorical_crossentropy: 0.9258 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00047: saving model to tmp/test/weights.47.h5\n",
      "\n",
      "Epoch 00047: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.92152\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7460 - categorical_crossentropy: 0.7460 - categorical_accuracy: 0.6224 - val_loss: 0.9247 - val_categorical_crossentropy: 0.9247 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00048: saving model to tmp/test/weights.48.h5\n",
      "\n",
      "Epoch 00048: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.92152\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7571 - categorical_crossentropy: 0.7571 - categorical_accuracy: 0.6224 - val_loss: 0.9245 - val_categorical_crossentropy: 0.9245 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00049: saving model to tmp/test/weights.49.h5\n",
      "\n",
      "Epoch 00049: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.92152\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7650 - categorical_crossentropy: 0.7650 - categorical_accuracy: 0.5964 - val_loss: 0.9248 - val_categorical_crossentropy: 0.9248 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00050: saving model to tmp/test/weights.50.h5\n",
      "\n",
      "Epoch 00050: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.92152\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7600 - categorical_crossentropy: 0.7600 - categorical_accuracy: 0.6354 - val_loss: 0.9228 - val_categorical_crossentropy: 0.9228 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00051: saving model to tmp/test/weights.51.h5\n",
      "\n",
      "Epoch 00051: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.92152\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7194 - categorical_crossentropy: 0.7194 - categorical_accuracy: 0.6380 - val_loss: 0.9240 - val_categorical_crossentropy: 0.9240 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 9.379477106108425e-10.\n",
      "\n",
      "Epoch 00052: saving model to tmp/test/weights.52.h5\n",
      "\n",
      "Epoch 00052: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.92152\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.8535 - categorical_crossentropy: 0.8535 - categorical_accuracy: 0.5807 - val_loss: 0.9247 - val_categorical_crossentropy: 0.9247 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00053: saving model to tmp/test/weights.53.h5\n",
      "\n",
      "Epoch 00053: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.92152\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7250 - categorical_crossentropy: 0.7250 - categorical_accuracy: 0.6562 - val_loss: 0.9224 - val_categorical_crossentropy: 0.9224 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00054: saving model to tmp/test/weights.54.h5\n",
      "\n",
      "Epoch 00054: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.92152\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7570 - categorical_crossentropy: 0.7570 - categorical_accuracy: 0.6276 - val_loss: 0.9256 - val_categorical_crossentropy: 0.9256 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00055: saving model to tmp/test/weights.55.h5\n",
      "\n",
      "Epoch 00055: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.92152\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7648 - categorical_crossentropy: 0.7648 - categorical_accuracy: 0.6354 - val_loss: 0.9253 - val_categorical_crossentropy: 0.9253 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00056: saving model to tmp/test/weights.56.h5\n",
      "\n",
      "Epoch 00056: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.92152\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7655 - categorical_crossentropy: 0.7655 - categorical_accuracy: 0.6328 - val_loss: 0.9261 - val_categorical_crossentropy: 0.9261 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00057: saving model to tmp/test/weights.57.h5\n",
      "\n",
      "Epoch 00057: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.92152\n",
      "Epoch 58/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7111 - categorical_crossentropy: 0.7111 - categorical_accuracy: 0.6797 - val_loss: 0.9261 - val_categorical_crossentropy: 0.9261 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 4.1457289201574097e-10.\n",
      "\n",
      "Epoch 00058: saving model to tmp/test/weights.58.h5\n",
      "\n",
      "Epoch 00058: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.92152\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7703 - categorical_crossentropy: 0.7703 - categorical_accuracy: 0.6328 - val_loss: 0.9254 - val_categorical_crossentropy: 0.9254 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00059: saving model to tmp/test/weights.59.h5\n",
      "\n",
      "Epoch 00059: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.92152\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7403 - categorical_crossentropy: 0.7403 - categorical_accuracy: 0.6484 - val_loss: 0.9262 - val_categorical_crossentropy: 0.9262 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00060: saving model to tmp/test/weights.60.h5\n",
      "\n",
      "Epoch 00060: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.92152\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7869 - categorical_crossentropy: 0.7869 - categorical_accuracy: 0.6250 - val_loss: 0.9239 - val_categorical_crossentropy: 0.9239 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00061: saving model to tmp/test/weights.61.h5\n",
      "\n",
      "Epoch 00061: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.92152\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.8014 - categorical_crossentropy: 0.8014 - categorical_accuracy: 0.6016 - val_loss: 0.9222 - val_categorical_crossentropy: 0.9222 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00062: saving model to tmp/test/weights.62.h5\n",
      "\n",
      "Epoch 00062: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.92152\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.8167 - categorical_crossentropy: 0.8167 - categorical_accuracy: 0.5990 - val_loss: 0.9220 - val_categorical_crossentropy: 0.9220 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00063: saving model to tmp/test/weights.63.h5\n",
      "\n",
      "Epoch 00063: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.92152\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7798 - categorical_crossentropy: 0.7798 - categorical_accuracy: 0.6302 - val_loss: 0.9217 - val_categorical_crossentropy: 0.9217 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 1.8324122347257443e-10.\n",
      "\n",
      "Epoch 00064: saving model to tmp/test/weights.64.h5\n",
      "\n",
      "Epoch 00064: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.92152\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7630 - categorical_crossentropy: 0.7630 - categorical_accuracy: 0.6224 - val_loss: 0.9212 - val_categorical_crossentropy: 0.9212 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00065: saving model to tmp/test/weights.65.h5\n",
      "\n",
      "Epoch 00065: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.92152 to 0.92122, saving model to tmp/test/best_loss.h5\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7373 - categorical_crossentropy: 0.7373 - categorical_accuracy: 0.6354 - val_loss: 0.9215 - val_categorical_crossentropy: 0.9215 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00066: saving model to tmp/test/weights.66.h5\n",
      "\n",
      "Epoch 00066: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.92122\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.7266 - categorical_crossentropy: 0.7266 - categorical_accuracy: 0.6458 - val_loss: 0.9213 - val_categorical_crossentropy: 0.9213 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00067: saving model to tmp/test/weights.67.h5\n",
      "\n",
      "Epoch 00067: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.92122\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.6899 - categorical_crossentropy: 0.6899 - categorical_accuracy: 0.6641 - val_loss: 0.9195 - val_categorical_crossentropy: 0.9195 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00068: saving model to tmp/test/weights.68.h5\n",
      "\n",
      "Epoch 00068: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.92122 to 0.91950, saving model to tmp/test/best_loss.h5\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 7s 617ms/step - loss: 0.7902 - categorical_crossentropy: 0.7902 - categorical_accuracy: 0.6198 - val_loss: 0.9197 - val_categorical_crossentropy: 0.9197 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00069: saving model to tmp/test/weights.69.h5\n",
      "\n",
      "Epoch 00069: val_categorical_accuracy did not improve from 0.53000\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.91950\n",
      "Epoch 70/70\n",
      " 3/12 [======>.......................] - ETA: 5s - loss: 0.7556 - categorical_crossentropy: 0.7556 - categorical_accuracy: 0.6771"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-da2bb727f3a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m                          \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                          \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                          validation_data=(x_val,y_val))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = densenet.get_compiled(optimizer=Adam(lr=1.e-3),\n",
    "                              loss='categorical_crossentropy',\n",
    "                              metrics=[\"categorical_accuracy\"])\n",
    "#from keras.models import load_model\n",
    "#model = load_model('wy.h5')\n",
    "\n",
    "filepath=\"best_weight.h5\"    \n",
    "save_folder='test'\n",
    "#reduce_lr = ReduceLROnPlateau(monitor='acc', factor=0.2,patience=2.9, min_lr=0.01)\n",
    "#callbacks=[reduce_lr]\n",
    "#early=EarlyStoppingByLossVal(monitor1='val_loss',monitor2='val_acc',value=2.6,verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath='tmp/%s/weights.{epoch:02d}.h5' % save_folder, verbose=1,\n",
    "                               period=1, save_weights_only=False)\n",
    "best_keeper = ModelCheckpoint(filepath='tmp/%s/best_acc.h5' % save_folder, verbose=1, save_weights_only=False,\n",
    "                              monitor='val_categorical_accuracy', save_best_only=True, period=1, mode='max')\n",
    "best_keeper2 = ModelCheckpoint(filepath='tmp/%s/best_loss.h5' % save_folder, verbose=1, save_weights_only=False,\n",
    "                              monitor='val_loss', save_best_only=True, period=1, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0, mode='max',patience=20, verbose=1)\n",
    "#checkpoint = ModelCheckpoint(filepath,monitor='val_loss',verbose=0,save_best_only=True,mode='min',period=1)\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.442, patience=5,\n",
    "                               verbose=1, mode='min', epsilon=1.e-5, cooldown=2, min_lr=0)\n",
    "#lrs=LearningRateScheduler(schedule=scheduler)\n",
    "#callbacks_list=[early_stopping, lr_reducer, checkpointer,best_keeper,best_keeper2]\n",
    "callbacks_list=[lr_reducer, checkpointer,best_keeper,best_keeper2]\n",
    "training_generator = MixupGenerator(x_train, y_train, batch_size=32, alpha=0.1)()\n",
    "model.fit_generator(generator=training_generator,\n",
    "                         steps_per_epoch=365//32+1,shuffle=True,\n",
    "                         epochs=70,\n",
    "                         callbacks=callbacks_list,\n",
    "                         validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  \n",
    "y_pred=model.predict(X_test_new)\n",
    "\n",
    "numpy.savetxt('new.csv',y_pred, delimiter = ',') \n",
    "model.save('first1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('tmp/test/weights.40.h5')\n",
    "import numpy  \n",
    "y_pred=model.predict(X_test_new)\n",
    "\n",
    "numpy.savetxt('new1.csv',y_pred, delimiter = ',') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
